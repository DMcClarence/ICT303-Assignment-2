{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu127\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tensorboard\n",
    "%pip install scikit-learn"
   ],
   "id": "924ddecfdff506ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Up\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from DataLoader import DataLoader\n",
    "from CNN import CNN\n",
    "from Trainer import Trainer\n",
    "\n",
    "def img_show(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    np_img = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(180),\n",
    "                                      transforms.RandomAdjustSharpness(0), # Randomly Blur Image\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "ca81adf861eac5b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_set = DataLoader(data_dir=os.getcwd() + \"/fire_dataset\", trans_width=224, trans_height=224).load(directory=\"/train\", transform=train_transform, batch_size=32, shuffle=True, workers=2)\n",
    "valid_set = DataLoader(data_dir=os.getcwd() + \"/fire_dataset\", trans_width=224, trans_height=224).load(transform=test_transform, batch_size=32, shuffle=True, workers=2)\n",
    "test_set = DataLoader(data_dir=os.getcwd() + \"/fire_dataset\", trans_width=224, trans_height=224).load(transform=test_transform, batch_size=32, shuffle=False, workers=2)\n",
    "model = CNN(log_dir=os.getcwd()+\"/CNN\", lr=1e-4).to(device)\n",
    "trainer = Trainer(log_dir=os.getcwd()+\"/CNN\", n_epochs=2, device=device)"
   ],
   "id": "a5a655cad7821e19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "writer = SummaryWriter(log_dir=os.getcwd()+\"/CNN\")",
   "id": "5accc0822c5770e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "trainer.fit(writer=writer, model=model, data=train_set, valid=valid_set, use_lr_scheduler=True)"
   ],
   "id": "b36a6467fb7ddf76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.clf()\n",
    "plt.plot(getattr(trainer, \"avg_train_loss\"), label=\"Avg. Training Loss\")\n",
    "plt.plot(getattr(trainer, \"avg_valid_loss\"), label=\"Avg. Validation Loss\")\n",
    "plt.plot(getattr(trainer, \"training_accuracy\"), label=\"Training Accuracy (x100)%\")\n",
    "plt.plot(getattr(trainer, \"validation_accuracy\"), label=\"Validation Accuracy (x100)%\")\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + \"/CNN/CNN_Loss_Plot.jpg\")"
   ],
   "id": "f9b948175372690a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%tensorboard --logdir=./CNN/CNN_Arch_1",
   "id": "2ace5aa7227e2f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classes = train_set.dataset.classes\n",
    "dataiter = iter(test_set)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "img_show(torchvision.utils.make_grid(images))\n",
    "model.eval()\n",
    "output = model(images).to(device)\n",
    "estimatedLabels = torch.max(output, 1).indices\n",
    "\n",
    "print('Estimated Labels: ', ' '.join(f'{classes[estimatedLabels[j]]:5s}' for j in range(images.shape[0])))"
   ],
   "id": "ce6d9d6f12f10ddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As someone who works in the Fire Detection and Evacuation industry, I thought it would be interesting to train a model that can recognise if there is a fire in a given image. So I've decided to train a model to perform binary classification on images that contain fire or no fire. I decided to do binary classification as it is something I am familiar with, allowing me to focus more on the process with finetuning without worrying too much about how to load and train the model.",
   "id": "d579ad94be4d3301"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As I already have built a data loader and trainer, and confusion matrix class in the first assignment, I brought that across and made any improvements or adjustments I needed to make. I also installed any libraries I required.",
   "id": "24e01cd4de09b0e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I have decided to move the image tranforms out of the data loader, making it a bit more modular by passing in the transformation to use. I also have update the transformation I will use for training by introducing some data augmentation such as random flips, rotation and sharpness. I added the sharpness augmentation to simulate the possibility of coming from low quality images, such as screenshots of CCTV footage or equivalent, allowing this to be applied in real world scenarios.",
   "id": "c9fc55a0ef57761f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Originally I was planning to try to implement a DenseNet architecture to address potential issues with exploding or vanishing gradients, however with the size of the dataset I have chosen, I think it may make the model too complex for the amount of data. So for the size of the dataset, it should require a smaller model. For this reason, I have decided to experiment around with some custom CNN architectures.",
   "id": "8cad4bb87b707a3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For my initial architecture, I have decided to use 3 blocks that consist of a convolutional layer, a ReLU activation and a max pool. I use ReLU as the activation function as it is the most commonly used activation function for modern neural networks. For the loss function, I have gone with torch's BCEWithLogitsLoss method. This method applies a Sigmoid activation to the outputs of the network and then calculates the Binary Cross Entropy Loss. According to torch's documentation, this is more stable than applying the Sigmoid manually and then calling BCELoss instead (https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).",
   "id": "67123791d96b20bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An issue I came across early was with my data loader. The dataset I chose didn't come with pre-organised training, validation, and test sets, so I knew I would have to split them myself. Instinctively, I did this in the data loader load method using the random_split method from torch. However, as I thought more deeply about it, I realised that if I needed to reload the dataset to continue training further, or restart training from a previous checkpoint, there would be no guarantees that I would have the same split, potentially causing the model to train with the test data. My solution was to split the dataset manually because of the size not being too large.",
   "id": "a7e3e804376dae9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I reorganised the dataset into 3 folders, one for training, one for validation, and one for testing. Inside the images are split into the 2 classes, fire and no fire. I used a 70/20/10 split for training, validation, and testing.",
   "id": "d116aec4b6146994"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
